{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Instalação de Dependências "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZWJbEoKq1gu",
        "outputId": "e8bbcf05-926f-436a-ff8f-ec684c1a6a75"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mO kernel não pôde ser iniciado porque \"TypeAliasType\" não pôde ser importado de \"/home/linguica/.venv/lib/python3.10/site-packages/typing_extensions.py\".\n",
            "\u001b[1;31mClique <a href=\"https://aka.ms/kernelFailuresModuleImportErrFromFile\">aqui</a> para obter mais informações."
          ]
        }
      ],
      "source": [
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mO kernel não pôde ser iniciado porque \"TypeAliasType\" não pôde ser importado de \"/home/linguica/.venv/lib/python3.10/site-packages/typing_extensions.py\".\n",
            "\u001b[1;31mClique <a href=\"https://aka.ms/kernelFailuresModuleImportErrFromFile\">aqui</a> para obter mais informações."
          ]
        }
      ],
      "source": [
        "%pip install pyarrow fastparquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CPGnWOHaq1gz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tratamento inical dos dados do arquivo 'FALHAS_PREDICT.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "h_b9xmFVq1g0",
        "outputId": "c92978f5-6dc4-4ee5-da56-849f1b2ea011"
      },
      "outputs": [],
      "source": [
        "df_falhas = pd.read_excel('FALHAS_PREDICT.xlsx')\n",
        "df_falhas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_falhas.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_falhas.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "yDtOofCLq1g5",
        "outputId": "78f08a99-5208-4ad9-862c-7fdf286f5075"
      },
      "outputs": [],
      "source": [
        "df_falhas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ajustando a linha correta como cabeçalho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "3fZH6o_Qq1g5",
        "outputId": "1380cda9-5686-4c6a-fd8a-9b8639829683"
      },
      "outputs": [],
      "source": [
        "# Definir a linha correta como cabeçalho\n",
        "df_falhas.columns = df_falhas.iloc[1]\n",
        "df_falhas = df_falhas[2:]\n",
        "\n",
        "# Resetar o índice do DataFrame\n",
        "df_falhas.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df_falhas.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZ0jx4Zuq1g6",
        "outputId": "4e862311-afd3-4ca9-d332-a3057d86b097"
      },
      "outputs": [],
      "source": [
        "df_falhas.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removendo coluna NaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFRH0WbZq1g6",
        "outputId": "7dd2ab9d-6f59-43eb-a653-10aee665f271"
      },
      "outputs": [],
      "source": [
        "# Remover a coluna `NaN`\n",
        "df_falhas.dropna(axis=1, how='all', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "SDRvu_duq1g7",
        "outputId": "b2e94c50-ec2e-415f-f5a7-716a181e2be9"
      },
      "outputs": [],
      "source": [
        "df_falhas.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removendo Duplicatas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR8zPR3ttq33",
        "outputId": "5c31f672-1572-49df-f6ca-3bc0cf1a4ae1"
      },
      "outputs": [],
      "source": [
        "df_falhas = df_falhas.drop_duplicates()\n",
        "df_falhas.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "X42rIHTCuRwO",
        "outputId": "0ec71c30-2e6d-424d-ad9f-f30726735a25"
      },
      "outputs": [],
      "source": [
        "df_falhas.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retirando colunas que não serão utilizadas nessa versão do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uK_AbattvMMx"
      },
      "outputs": [],
      "source": [
        "# Dropar colunas\n",
        "df_falhas.drop(['MODELO', 'COR', 'MOTOR', 'ESTACAO', 'USUARIO'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tYJRjqzbwR-3",
        "outputId": "467c74f1-d947-4235-efaf-6621da0744ef"
      },
      "outputs": [],
      "source": [
        "df_falhas.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filtrando ZPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N_3UOLPGwZ3V"
      },
      "outputs": [],
      "source": [
        "# Filtrando para manter apenas valores entre ZP1 e ZP7\n",
        "df_falhas = df_falhas[df_falhas['HALLE'].str.match(r'ZP[1-7]', na=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IhiXkKFixFuC",
        "outputId": "a2573586-95e6-467b-f587-14096766523b"
      },
      "outputs": [],
      "source": [
        "df_falhas.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gerando um novo arquivo do dataset utilizando parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-knx079Fxp3g"
      },
      "outputs": [],
      "source": [
        "df_falhas.to_parquet('df_falhas.parquet', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_falhas = pd.read_parquet('df_falhas.parquet')\n",
        "df_falhas.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTtRIX_16BtC"
      },
      "source": [
        "# Tratamento e Ajustes nos dados de resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RESULTADOS 02"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nas células a seguir foram selecionadas as colunas necessarias para o desenvolvimento desse modelo e o tratamento para que não existam dados duplicados. Por fim, foi gerado um novo dataset utilizando parquet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02 = pd.read_excel('RESULTADOS_02_03_2024.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df_result02.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02.columns = df_result02.iloc[0]\n",
        "df_result02 = df_result02[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df_result02.columns = df_result02.columns.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02 = df_result02.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identificar todas as duplicatas\n",
        "duplicates = df_result02[df_result02.duplicated(keep=False)]\n",
        "\n",
        "print(duplicates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02 = df_result02.drop_duplicates()\n",
        "df_result02.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02.to_parquet('df_result02.parquet', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02 = pd.read_parquet('df_result02.parquet')\n",
        "df_result02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02 = df_result02.loc[:, df_result02.columns != 'nan']\n",
        "df_result02 = df_result02.loc[:, df_result02.columns != 'ID']\n",
        "df_result02 = df_result02.loc[:, df_result02.columns != 'STATUS']\n",
        "df_result02\n",
        "\n",
        "#remover colunas inuteis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_unicos = df_result02['NAME'].nunique()\n",
        "print(f\"A coluna 'NAME' possui {num_unicos} valores únicos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "valores_unicos = df_result02['NAME'].unique()\n",
        "print(\"Valores únicos na coluna 'NAME':\")\n",
        "print(valores_unicos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frequencia = df_result02['NAME'].value_counts()\n",
        "print(\"Frequência de cada valor único na coluna 'NAME':\")\n",
        "print(frequencia)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explorando Value_id e Name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import f_oneway\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agrupar por 'NAME' e calcular estatísticas descritivas de 'VALUE_ID'\n",
        "estatisticas = df_result02.groupby('NAME')['VALUE_ID'].describe()\n",
        "print(estatisticas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='NAME', y='VALUE_ID', data=df_result02)\n",
        "plt.title('Distribuição de VALUE_ID por NAME')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "medias = df_result02.groupby('NAME')['VALUE_ID'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='NAME', y='VALUE_ID', data=medias)\n",
        "plt.title('Média de VALUE_ID por NAME')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar lista de valores de 'VALUE_ID' para cada grupo de 'NAME'\n",
        "grupos = [grupo['VALUE_ID'].dropna().values for nome, grupo in df_result02.groupby('NAME')]\n",
        "\n",
        "# Realizar o teste ANOVA\n",
        "estatistica, p_valor = f_oneway(*grupos)\n",
        "print(f'Estatística F: {estatistica}')\n",
        "print(f'Valor-p: {p_valor}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explorando Unity, Value e ValueID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar os tipos de dados\n",
        "print(df_result02[['UNIT', 'VALUE', 'VALUE_ID']].dtypes)\n",
        "\n",
        "# Converter 'VALUE' e 'VALUE_ID' para numérico, se necessário\n",
        "df_result02['VALUE'] = pd.to_numeric(df_result02['VALUE'], errors='coerce')\n",
        "df_result02['VALUE_ID'] = pd.to_numeric(df_result02['VALUE_ID'], errors='coerce')\n",
        "\n",
        "# Remover valores nulos nas colunas de interesse\n",
        "df_result02 = df_result02.dropna(subset=['UNIT', 'VALUE', 'VALUE_ID'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estatísticas descritivas de 'VALUE'\n",
        "print(\"Estatísticas de 'VALUE':\")\n",
        "print(df_result02['VALUE'].describe())\n",
        "\n",
        "# Estatísticas descritivas de 'VALUE_ID'\n",
        "print(\"\\nEstatísticas de 'VALUE_ID':\")\n",
        "print(df_result02['VALUE_ID'].describe())\n",
        "\n",
        "# Valores únicos em 'UNIT'\n",
        "print(\"\\nValores únicos em 'UNIT':\")\n",
        "print(df_result02['UNIT'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df_result02['VALUE_ID'], df_result02['VALUE'], alpha=0.5)\n",
        "plt.title('Relação entre VALUE_ID e VALUE')\n",
        "plt.xlabel('VALUE_ID')\n",
        "plt.ylabel('VALUE')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "df_result02.boxplot(column='VALUE', by='UNIT', grid=False)\n",
        "plt.title('Distribuição de VALUE por UNIT')\n",
        "plt.suptitle('')\n",
        "plt.xlabel('UNIT')\n",
        "plt.ylabel('VALUE')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histograma de VALUE\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df_result02['VALUE'], bins=30, alpha=0.7)\n",
        "plt.title('Distribuição de VALUE')\n",
        "plt.xlabel('VALUE')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n",
        "\n",
        "# Histograma de VALUE_ID\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(df_result02['VALUE_ID'], bins=30, alpha=0.7, color='orange')\n",
        "plt.title('Distribuição de VALUE_ID')\n",
        "plt.xlabel('VALUE_ID')\n",
        "plt.ylabel('Frequência')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Matriz de correlação\n",
        "corr = df_result02[['VALUE', 'VALUE_ID']].corr()\n",
        "print(\"Matriz de correlação:\")\n",
        "print(corr)\n",
        "\n",
        "# Heatmap da correlação\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Correlação entre VALUE e VALUE_ID')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agrupamento = df_result02.groupby('UNIT')[['VALUE', 'VALUE_ID']].mean()\n",
        "print(\"\\nMédias de VALUE e VALUE_ID por UNIT:\")\n",
        "print(agrupamento)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='VALUE_ID', y='VALUE', hue='UNIT', data=df_result02)\n",
        "plt.title('Relação entre VALUE_ID e VALUE colorido por UNIT')\n",
        "plt.xlabel('VALUE_ID')\n",
        "plt.ylabel('VALUE')\n",
        "plt.legend(title='UNIT', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converter 'VALUE' e 'VALUE_ID' para numérico\n",
        "df_result02['VALUE'] = pd.to_numeric(df_result02['VALUE'], errors='coerce')\n",
        "df_result02['VALUE_ID'] = pd.to_numeric(df_result02['VALUE_ID'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remover linhas com valores nulos em 'VALUE' ou 'VALUE_ID'\n",
        "df_result02 = df_result02.dropna(subset=['VALUE', 'VALUE_ID'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_unique_value = df_result02['VALUE'].nunique()\n",
        "num_unique_value_id = df_result02['VALUE_ID'].nunique()\n",
        "\n",
        "print(f\"Número de valores únicos em 'VALUE': {num_unique_value}\")\n",
        "print(f\"Número de valores únicos em 'VALUE_ID': {num_unique_value_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajustar 'q' com base no número de valores únicos\n",
        "q_value = min(4, num_unique_value)\n",
        "q_value_id = min(4, num_unique_value_id)\n",
        "\n",
        "# Categorizar 'VALUE'\n",
        "df_result02['VALUE_cat'] = pd.qcut(df_result02['VALUE'], q=q_value, labels=False, duplicates='drop')\n",
        "\n",
        "# Categorizar 'VALUE_ID'\n",
        "df_result02['VALUE_ID_cat'] = pd.qcut(df_result02['VALUE_ID'], q=q_value_id, labels=False, duplicates='drop')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_result02['VALUE_cat'].isnull().sum())\n",
        "print(df_result02['VALUE_ID_cat'].isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Criar a matriz de confusão\n",
        "conf_mat = confusion_matrix(df_result02['VALUE_ID_cat'], df_result02['VALUE_cat'])\n",
        "\n",
        "# Exibir a matriz de confusão\n",
        "print(\"\\nMatriz de Confusão entre VALUE_ID_cat e VALUE_cat:\")\n",
        "print(conf_mat)\n",
        "\n",
        "# Visualizar a matriz de confusão\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Q1', 'Q2', 'Q3', 'Q4'],\n",
        "            yticklabels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
        "plt.xlabel('VALUE_cat')\n",
        "plt.ylabel('VALUE_ID_cat')\n",
        "plt.title('Matriz de Confusão entre VALUE_ID_cat e VALUE_cat')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codificar 'UNIT' numericamente\n",
        "df_result02['UNIT_code'] = df_result02['UNIT'].astype('category').cat.codes\n",
        "\n",
        "conf_mat_unit = confusion_matrix(df_result02['UNIT_code'], df_result02['VALUE_cat'])\n",
        "\n",
        "unit_labels = df_result02['UNIT'].astype('category').cat.categories\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(conf_mat_unit, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Q1', 'Q2', 'Q3', 'Q4'],\n",
        "            yticklabels=unit_labels)\n",
        "plt.xlabel('VALUE_cat')\n",
        "plt.ylabel('UNIT')\n",
        "plt.title('Matriz de Confusão entre UNIT e VALUE_cat')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploração dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02 = pd.read_excel('RESULTADOS_02_03_2024.xlsx')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02.columns\n",
        "df_result02.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02.columns = df_result02.iloc[0]\n",
        "df_result02 = df_result02[1:]\n",
        "df_result02.columns = df_result02.columns.astype(str)\n",
        "df_result02 = df_result02.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02 = df_result02.loc[:, df_result02.columns != 'nan']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "unique_names = df_result02['NAME'].unique()\n",
        "\n",
        "print(\"Valores únicos na coluna 'NAME':\")\n",
        "print(unique_names)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_unique_names = df_result02['NAME'].nunique()\n",
        "print(f\"Número de valores únicos na coluna 'NAME': {num_unique_names}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_value_ids = df_result02['VALUE_ID'].unique()\n",
        "\n",
        "print(\"Valores únicos na coluna 'VALUE_ID':\")\n",
        "print(unique_value_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_unique_value_ids = df_result02['VALUE_ID'].nunique()\n",
        "print(f\"Número de valores únicos na coluna 'VALUE_ID': {num_unique_value_ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_null_names = df_result02['NAME'].isnull().sum()\n",
        "print(f\"Número de valores nulos na coluna 'NAME': {num_null_names}\")\n",
        "\n",
        "\n",
        "num_null_value_ids = df_result02['VALUE_ID'].isnull().sum()\n",
        "print(f\"Número de valores nulos na coluna 'VALUE_ID': {num_null_value_ids}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result02 = df_result02.dropna(subset=['VALUE_ID'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_names_sorted = sorted(unique_names)\n",
        "unique_value_ids_sorted = sorted(unique_value_ids)\n",
        "\n",
        "print(\"Valores únicos na coluna 'NAME' (ordenados):\")\n",
        "print(unique_names_sorted)\n",
        "\n",
        "print(\"Valores únicos na coluna 'VALUE_ID' (ordenados):\")\n",
        "print(unique_value_ids_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agrupar por 'NAME' e 'VALUE_ID' e contar as ocorrências\n",
        "contagens_name_valueid = df_result02.groupby(['NAME', 'VALUE_ID']).size().reset_index(name='counts')\n",
        "\n",
        "# Exibir as contagens\n",
        "print(contagens_name_valueid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filtrar as combinações onde counts > 1\n",
        "combinacoes_repetidas = contagens_name_valueid[contagens_name_valueid['counts'] > 1]\n",
        "\n",
        "# Exibir as combinações repetidas\n",
        "print(\"Combinações repetidas de 'NAME' e 'VALUE_ID':\")\n",
        "print(combinacoes_repetidas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ordenar as combinações por 'counts' em ordem decrescente\n",
        "contagens_ordenadas = contagens_name_valueid.sort_values(by='counts', ascending=False)\n",
        "\n",
        "print(\"Todas as combinações de 'NAME' e 'VALUE_ID' ordenadas por contagem:\")\n",
        "print(contagens_ordenadas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar a tabela dinâmica\n",
        "tabela_pivot = contagens_name_valueid.pivot(index='NAME', columns='VALUE_ID', values='counts').fillna(0)\n",
        "\n",
        "# Exibir a tabela\n",
        "print(tabela_pivot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar o número de 'VALUE_ID's únicos para cada 'NAME'\n",
        "contagem_valueid_por_name = df_result02.groupby('NAME')['VALUE_ID'].nunique().reset_index(name='unique_valueid_count')\n",
        "\n",
        "# Filtrar os 'NAME's que têm mais de um 'VALUE_ID' único\n",
        "names_multiplos_valueid = contagem_valueid_por_name[contagem_valueid_por_name['unique_valueid_count'] > 1]\n",
        "\n",
        "print(\"Nomes associados a múltiplos 'VALUE_ID's:\")\n",
        "print(names_multiplos_valueid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar o número de 'NAME's únicos para cada 'VALUE_ID'\n",
        "contagem_name_por_valueid = df_result02.groupby('VALUE_ID')['NAME'].nunique().reset_index(name='unique_name_count')\n",
        "\n",
        "# Filtrar os 'VALUE_ID's que têm mais de um 'NAME' único\n",
        "valueid_multiplos_names = contagem_name_por_valueid[contagem_name_por_valueid['unique_name_count'] > 1]\n",
        "\n",
        "print(\"VALUE_ID's associados a múltiplos 'NAME's:\")\n",
        "print(valueid_multiplos_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agrupar por 'NAME' e 'VALUE_ID' e contar as ocorrências\n",
        "contagens_name_valueid = df_result02.groupby(['NAME', 'VALUE_ID']).size().reset_index(name='Counts')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ordenar a tabela por 'Counts' em ordem decrescente\n",
        "contagens_name_valueid_sorted = contagens_name_valueid.sort_values(by='Counts', ascending=False)\n",
        "\n",
        "# Exibir a tabela ordenada\n",
        "print(\"Tabela Ordenada de Contagens para 'NAME' e 'VALUE_ID':\")\n",
        "print(contagens_name_valueid_sorted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar a tabela dinâmica\n",
        "pivot_table = contagens_name_valueid.pivot(index='NAME', columns='VALUE_ID', values='Counts').fillna(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Inicializar os codificadores de labels\n",
        "le_name = LabelEncoder()\n",
        "le_value_id = LabelEncoder()\n",
        "\n",
        "# Ajustar e transformar as colunas\n",
        "df_result02['NAME_encoded'] = le_name.fit_transform(df_result02['NAME'])\n",
        "df_result02['VALUE_ID_encoded'] = le_value_id.fit_transform(df_result02['VALUE_ID'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Calcular a matriz de confusão\n",
        "conf_matrix = confusion_matrix(df_result02['NAME_encoded'], df_result02['VALUE_ID_encoded'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecionar os top N 'NAME's mais frequentes\n",
        "top_n_names = df_result02['NAME'].value_counts().nlargest(10).index\n",
        "df_top = df_result02[df_result02['NAME'].isin(top_n_names)]\n",
        "\n",
        "# Repetir o processo de codificação e plotagem com df_top\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RESULTADOS 04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nas células a seguir foram selecionadas as colunas necessarias para o desenvolvimento desse modelo e o tratamento para que não existam dados duplicados. Por fim, foi gerado um novo dataset utilizando parquet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result04 = pd.read_csv('RESULTADOS_04_06_2024_full_teste.csv', low_memory=False, compression='gzip')\n",
        "df_result04.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dropar uma coluna e modificar o DataFrame existente\n",
        "df_result04.drop(['Unnamed: 0', 'NAME', 'STATUS', 'UNIT', 'VALUE_ID', 'VALUE'], axis=1, inplace=True)\n",
        "df_result04.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result04 = df_result04.drop_duplicates()\n",
        "df_result04"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mO kernel não pôde ser iniciado porque \"TypeAliasType\" não pôde ser importado de \"/home/linguica/.venv/lib/python3.10/site-packages/typing_extensions.py\".\n",
            "\u001b[1;31mClique <a href=\"https://aka.ms/kernelFailuresModuleImportErrFromFile\">aqui</a> para obter mais informações."
          ]
        }
      ],
      "source": [
        "df_result04.to_parquet('df_result04.parquet', index=False)\n",
        "df_result04 = pd.read_parquet('df_result04.parquet')\n",
        "df_result04"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extração de Features \n",
        "## Name e Value_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mO kernel não pôde ser iniciado porque \"TypeAliasType\" não pôde ser importado de \"/home/linguica/.venv/lib/python3.10/site-packages/typing_extensions.py\".\n",
            "\u001b[1;31mClique <a href=\"https://aka.ms/kernelFailuresModuleImportErrFromFile\">aqui</a> para obter mais informações."
          ]
        }
      ],
      "source": [
        "# Exibir os primeiros valores de 'NAME'\n",
        "print(df_result02['NAME'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dividir 'NAME' em partes\n",
        "df_result02[['PART1', 'PART2', 'PART3']] = df_result02['NAME'].str.split('_', expand=True)\n",
        "\n",
        "# Exibir as novas colunas\n",
        "print(df_result02[['NAME', 'PART1', 'PART2', 'PART3']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converter 'PART3' para numérico\n",
        "df_result02['PART3_num'] = pd.to_numeric(df_result02['PART3'], errors='coerce')\n",
        "\n",
        "# Exibir a nova coluna\n",
        "print(df_result02[['PART3', 'PART3_num']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-Hot Encoding para 'PART1' e 'PART2'\n",
        "df_encoded = pd.get_dummies(df_result02, columns=['PART1', 'PART2'])\n",
        "\n",
        "# Exibir as novas colunas\n",
        "print(df_encoded.filter(regex='PART1_|PART2_').head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Inicializar o codificador\n",
        "le_part1 = LabelEncoder()\n",
        "le_part2 = LabelEncoder()\n",
        "\n",
        "# Codificar as colunas\n",
        "df_result02['PART1_encoded'] = le_part1.fit_transform(df_result02['PART1'])\n",
        "df_result02['PART2_encoded'] = le_part2.fit_transform(df_result02['PART2'])\n",
        "\n",
        "# Exibir as novas colunas\n",
        "print(df_result02[['PART1', 'PART1_encoded', 'PART2', 'PART2_encoded']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exibir valores únicos em 'VALUE_ID'\n",
        "print(df_result02['VALUE_ID'].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Normalização\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df_result02['VALUE_ID_norm'] = scaler_minmax.fit_transform(df_result02[['VALUE_ID']])\n",
        "\n",
        "# Padronização\n",
        "scaler_standard = StandardScaler()\n",
        "df_result02['VALUE_ID_std'] = scaler_standard.fit_transform(df_result02[['VALUE_ID']])\n",
        "\n",
        "# Exibir as novas colunas\n",
        "print(df_result02[['VALUE_ID', 'VALUE_ID_norm', 'VALUE_ID_std']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar categorias usando pd.cut\n",
        "df_result02['VALUE_ID_cat'] = pd.cut(df_result02['VALUE_ID'], bins=4, labels=False)\n",
        "\n",
        "# Exibir a nova coluna\n",
        "print(df_result02[['VALUE_ID', 'VALUE_ID_cat']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar uma nova coluna combinando 'NAME' e 'VALUE_ID'\n",
        "df_result02['NAME_VALUEID'] = df_result02['NAME'] + '_' + df_result02['VALUE_ID'].astype(str)\n",
        "\n",
        "# Exibir a nova coluna\n",
        "print(df_result02[['NAME', 'VALUE_ID', 'NAME_VALUEID']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label Encoding\n",
        "le_name_valueid = LabelEncoder()\n",
        "df_result02['NAME_VALUEID_encoded'] = le_name_valueid.fit_transform(df_result02['NAME_VALUEID'])\n",
        "\n",
        "# Exibir a nova coluna\n",
        "print(df_result02[['NAME_VALUEID', 'NAME_VALUEID_encoded']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converter 'DATA' para datetime\n",
        "df_result02['DATA'] = pd.to_datetime(df_result02['DATA'], errors='coerce')\n",
        "\n",
        "# Exibir a coluna convertida\n",
        "print(df_result02['DATA'].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extrair ano, mês, dia, hora, dia da semana\n",
        "df_result02['ANO'] = df_result02['DATA'].dt.year\n",
        "df_result02['MES'] = df_result02['DATA'].dt.month\n",
        "df_result02['DIA'] = df_result02['DATA'].dt.day\n",
        "df_result02['HORA'] = df_result02['DATA'].dt.hour\n",
        "df_result02['DIA_SEMANA'] = df_result02['DATA'].dt.dayofweek  # 0 = segunda-feira\n",
        "\n",
        "# Exibir as novas colunas\n",
        "print(df_result02[['DATA', 'ANO', 'MES', 'DIA', 'HORA', 'DIA_SEMANA']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar frequência de cada 'NAME'\n",
        "name_counts = df_result02['NAME'].value_counts()\n",
        "\n",
        "# Mapear a frequência para o DataFrame\n",
        "df_result02['NAME_freq'] = df_result02['NAME'].map(name_counts)\n",
        "\n",
        "# Exibir a nova coluna\n",
        "print(df_result02[['NAME', 'NAME_freq']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar frequência de cada 'VALUE_ID'\n",
        "valueid_counts = df_result02['VALUE_ID'].value_counts()\n",
        "\n",
        "# Mapear a frequência para o DataFrame\n",
        "df_result02['VALUE_ID_freq'] = df_result02['VALUE_ID'].map(valueid_counts)\n",
        "\n",
        "# Exibir a nova coluna\n",
        "print(df_result02[['VALUE_ID', 'VALUE_ID_freq']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir um limiar (por exemplo, média de 'VALUE_ID')\n",
        "limiar = df_result02['VALUE_ID'].mean()\n",
        "\n",
        "# Criar a feature binária\n",
        "df_result02['VALUE_ID_high'] = df_result02['VALUE_ID'].apply(lambda x: 1 if x > limiar else 0)\n",
        "\n",
        "# Exibir a nova coluna\n",
        "print(df_result02[['VALUE_ID', 'VALUE_ID_high']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular a média de 'VALUE_ID' para cada 'NAME'\n",
        "name_target_mean = df_result02.groupby('NAME')['VALUE_ID'].mean()\n",
        "\n",
        "# Mapear a média para o DataFrame\n",
        "df_result02['NAME_target_mean'] = df_result02['NAME'].map(name_target_mean)\n",
        "\n",
        "# Exibir a nova coluna\n",
        "print(df_result02[['NAME', 'NAME_target_mean']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar valores nulos\n",
        "print(\"Valores nulos por coluna:\")\n",
        "print(df_result02.isnull().sum())\n",
        "\n",
        "# Opcional: Preencher valores nulos ou remover linhas com nulos\n",
        "df_result02 = df_result02.dropna()  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Passo 1: Agrupar por 'NAME' e 'VALUE_ID' e contar ocorrências\n",
        "name_valueid_counts = df_result02.groupby(['NAME', 'VALUE_ID']).size().reset_index(name='Counts')\n",
        "\n",
        "# Passo 2: Identificar combinações repetidas\n",
        "repeating_combinations = name_valueid_counts[name_valueid_counts['Counts'] > 1]\n",
        "\n",
        "# Passo 3: Mesclar com informações de 'KNR'\n",
        "repeating_combinations_knr = pd.merge(repeating_combinations, df_result02[['NAME', 'VALUE_ID', 'KNR']], on=['NAME', 'VALUE_ID'], how='left')\n",
        "\n",
        "# Passo 4: Remover duplicatas\n",
        "repeating_combinations_knr = repeating_combinations_knr.drop_duplicates(subset=['NAME', 'VALUE_ID', 'KNR'])\n",
        "\n",
        "# Passo 5: Criar mapeamento com o 'KNR' mais frequente\n",
        "most_frequent_knr = df_result02.groupby(['NAME', 'VALUE_ID'])['KNR'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
        "name_valueid_to_knr = most_frequent_knr.set_index(['NAME', 'VALUE_ID'])['KNR'].to_dict()\n",
        "\n",
        "# Passo 6: Adicionar nova coluna ao DataFrame\n",
        "df_result02['KNR_associated'] = df_result02.apply(lambda row: name_valueid_to_knr.get((row['NAME'], row['VALUE_ID']), None), axis=1)\n",
        "\n",
        "# Passo 7: Verificar os resultados\n",
        "df_result02['KNR_match'] = df_result02['KNR'] == df_result02['KNR_associated']\n",
        "discrepancies = df_result02[df_result02['KNR_match'] == False]\n",
        "\n",
        "# Exibir o DataFrame final\n",
        "print(df_result02[['NAME', 'VALUE_ID', 'KNR', 'KNR_associated', 'KNR_match']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RESULTADOS 06"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nas células a seguir foram selecionadas as colunas necessarias para o desenvolvimento desse modelo e o tratamento para que não existam dados duplicados. Por fim, foi gerado um novo dataset utilizando parquet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result06 = pd.read_csv('RESULTADOS_06_2023_07_2023_full_teste.csv', low_memory=False, compression='gzip')\n",
        "df_result06.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dropar uma coluna e modificar o DataFrame existente\n",
        "df_result06.drop(['Unnamed: 0', 'NAME', 'STATUS', 'UNIT', 'VALUE_ID', 'VALUE'], axis=1, inplace=True)\n",
        "df_result06.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result06 = df_result06.drop_duplicates()\n",
        "df_result06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result06.to_parquet('df_result06.parquet', index=False)\n",
        "df_result06 = pd.read_parquet('df_result06.parquet')\n",
        "df_result06"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RESULTADOS 08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nas células a seguir foram selecionadas as colunas necessarias para o desenvolvimento desse modelo e o tratamento para que não existam dados duplicados. Por fim, foi gerado um novo dataset utilizando parquet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result08 = pd.read_csv('RESULTADOS_08_2023_09_2023_full_teste.csv', low_memory=False, compression='gzip')\n",
        "df_result08.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dropar uma coluna e modificar o DataFrame existente\n",
        "df_result08.drop(['Unnamed: 0', 'NAME', 'STATUS', 'UNIT', 'VALUE_ID', 'VALUE'], axis=1, inplace=True)\n",
        "df_result08.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result08 = df_result08.drop_duplicates()\n",
        "df_result08"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result08.to_parquet('df_result08.parquet', index=False)\n",
        "df_result08 = pd.read_parquet('df_result08.parquet')\n",
        "df_result08"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RESULTADOS 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Nas células a seguir foram selecionadas as colunas necessarias para o desenvolvimento desse modelo e o tratamento para que não existam dados duplicados. Por fim, foi gerado um novo dataset utilizando parquet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result10 = pd.read_csv('RESULTADOS_10_2023_11_2023_full_teste.csv', low_memory=False, compression='gzip')\n",
        "df_result10.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dropar uma coluna e modificar o DataFrame existente\n",
        "df_result10.drop(['Unnamed: 0', 'NAME', 'STATUS', 'UNIT', 'VALUE_ID', 'VALUE'], axis=1, inplace=True)\n",
        "df_result10.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result10 = df_result10.drop_duplicates()\n",
        "df_result10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result10.to_parquet('df_result10.parquet', index=False)\n",
        "df_result10 = pd.read_parquet('df_result10.parquet')\n",
        "df_result10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MERGE DOS ARQUIVOS RESULT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nas células a seguir, foi realizado a junção dos dados das tabelas de resultado em um único arquivo para uso do conjunto de dados na construção do novo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Lista com os nomes dos arquivos Parquet\n",
        "arquivos = [\n",
        "    'df_result02.parquet',\n",
        "    'df_result04.parquet',\n",
        "    'df_result06.parquet',\n",
        "    'df_result08.parquet',\n",
        "    'df_result10.parquet'\n",
        "]\n",
        "\n",
        "# Inicializar uma lista vazia para armazenar os DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Ler cada arquivo Parquet em um DataFrame e adicionar à lista\n",
        "for arquivo in arquivos:\n",
        "    df = pd.read_parquet(arquivo)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Concatenar todos os DataFrames em um único DataFrame\n",
        "df_merged = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Salvar o DataFrame resultante em um novo arquivo Parquet\n",
        "df_merged.to_parquet('df_merged.parquet')\n",
        "\n",
        "print(\"Merge concluído e salvo como 'df_merged.parquet'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged = pd.read_parquet('df_merged.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MERGE DOS ARQUIVOS RESULT E FAALHAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nessa célula foi realizada a junção de todos os dados necessários para a construção do modelo da sprint 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Fazer a combinação usando a coluna comum 'KNR'\n",
        "# # Nesse caso, estamos apenas combinando a coluna 'HALLE' da tabela df_falhas_predict com df_result\n",
        "df_combined = pd.merge(df_merged, df_falhas[['KNR', 'HALLE', 'FALHA']], on='KNR', how='inner')\n",
        "\n",
        "# # Visualizar o DataFrame resultante\n",
        "print(df_combined.head())\n",
        "\n",
        "# # Se necessário, salvar o resultado em um novo arquivo CSV\n",
        "df_combined.to_parquet(\"df_result_combined.parqut\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result_combined = pd.read_parquet('df_result_combined.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result_combined.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result_combined.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aqui foram realizadas diversas tentativas para padrodizar os dados da coluna 'DATA' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primeiro, tenta converter diretamente, assumindo que a maioria dos valores tenha milissegundos\n",
        "df_result_combined['DATA_FORMATADA'] = pd.to_datetime(df_result_combined['DATA'], format=\"%Y-%m-%d %H:%M:%S.%f\", errors='coerce')\n",
        "\n",
        "# Agora, lida com aqueles que falharam (são NaT) e tenta converter sem milissegundos\n",
        "df_result_combined['DATA_FORMATADA'].fillna(pd.to_datetime(df_result_combined['DATA'], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce'), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result_combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_result_combined['HALLE'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Ordenar por KNR e DATA_FORMATADA\n",
        "df_result_combined = df_result_combined.sort_values(by=['KNR', 'DATA_FORMATADA'])\n",
        "\n",
        "# Calcular o tempo entre falhas consecutivas dentro do mesmo carro e estação (HALLE)\n",
        "df_result_combined['TEMPO'] = df_result_combined.groupby(['KNR', 'HALLE'])['DATA_FORMATADA'].diff().fillna(pd.Timedelta(seconds=0))\n",
        "\n",
        "# Somar o tempo gasto em cada estação (HALLE) por cada carro (KNR)\n",
        "tempo_por_estacao = df_result_combined.groupby(['KNR', 'HALLE'])['TEMPO'].sum().reset_index()\n",
        "\n",
        "# Criar um DataFrame pivotado para armazenar a soma de tempo em colunas separadas\n",
        "resultado = tempo_por_estacao.pivot(index='KNR', columns='HALLE', values='TEMPO').fillna(pd.Timedelta(seconds=0))\n",
        "\n",
        "# Renomear as colunas para o formato desejado (ajustando para HALLE correto)\n",
        "resultado.columns = ['SomaTempo1' if col == 'ZP5A' else 'SomaTempo2' if col == 'ZP5B' else 'SomaTempo718' for col in resultado.columns]\n",
        "\n",
        "# Resetar o índice para transformar KNR em coluna\n",
        "resultado = resultado.reset_index()\n",
        "\n",
        "# Exibir o resultado final\n",
        "print(resultado)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultado.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultado"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
